# Performance Optimization Plan - Snap-and-Solve

**Based on Actual Performance Data (Jan 7, 2026)**

## ğŸ“Š Current Performance Breakdown

```
Total Time: 17,403ms (17.4 seconds)

Breakdown:
- Firebase Storage upload: 543ms (3.1%)     âœ… GOOD
- OpenAI Vision API call: 13,393ms (77.0%)  ğŸ”´ MAIN BOTTLENECK
- Firestore save: 2,358ms (13.5%)           âš ï¸  TOO SLOW
- Usage check: 1,107ms (6.4%)               âš ï¸  TOO SLOW
```

**Target: Reduce to 8-12 seconds (30-50% improvement)**

---

## ğŸ¯ Optimization Strategy

### **Priority 1: Parallel Firebase Operations** â­ QUICK WIN
**Expected Savings: 1-2 seconds (6-12%)**
**Effort: 1 hour**
**Risk: Low**

#### Current Flow (Sequential):
```javascript
// solve.js (line 210-227)
1. Save snap record to Firestore         â†’ 2,358ms
   - Add snap document                    â†’ ~1,200ms
   - Run transaction (increment usage)   â†’ ~1,158ms
2. Get updated usage from Firestore      â†’ 1,107ms
Total: 3,465ms
```

#### Optimized Flow (Parallel):
```javascript
// Run save + usage check in parallel
const [snapId, updatedUsage] = await Promise.all([
  saveSnapRecord(...),
  getDailyUsage(userId)
]);
// Total: max(2,358ms, 1,107ms) = 2,358ms
// Savings: 1,107ms
```

#### Implementation:

**File: `backend/src/routes/solve.js` (lines 209-227)**

```javascript
// BEFORE (Sequential):
const firestoreSaveStart = Date.now();
const snapId = await saveSnapRecord(userId, {...});
perfSteps.firestoreSave = Date.now() - firestoreSaveStart;

const usageStartTime = Date.now();
const updatedUsage = await getDailyUsage(userId);
perfSteps.usageCheck = Date.now() - usageStartTime;

// AFTER (Parallel):
const firestoreStart = Date.now();
const [snapId, updatedUsage] = await Promise.all([
  saveSnapRecord(userId, {
    recognizedQuestion: solutionData.recognizedQuestion,
    subject: solutionData.subject,
    topic: solutionData.topic,
    difficulty: solutionData.difficulty,
    language: solutionData.language,
    solution: solutionData.solution,
    imageUrl: imageUrl,
    requestId: req.id
  }),
  getDailyUsage(userId)
]);
perfSteps.firestoreOperations = Date.now() - firestoreStart;
logger.info(`â±ï¸  [PERF] Firestore operations (parallel): ${perfSteps.firestoreOperations}ms`, { requestId: req.id });
```

**Expected Result:**
- Before: 3,465ms (save + usage check sequential)
- After: ~2,400ms (parallel, takes max of the two)
- **Savings: ~1,100ms (6.3%)**

---

### **Priority 2: Switch to GPT-4o-mini for Vision API** â­ BIG WIN
**Expected Savings: 4-6 seconds (25-35%)**
**Effort: 15 minutes**
**Risk: Medium (need to test accuracy)**

#### Why GPT-4o-mini?

| Model | Speed | Accuracy | Cost | Use Case |
|-------|-------|----------|------|----------|
| **gpt-4o** (current) | 13-15s | Excellent | High | Complex reasoning |
| **gpt-4o-mini** | 6-9s | Very Good | Low | Educational content |
| gpt-4-turbo | 8-11s | Excellent | Medium | Balanced |

**GPT-4o-mini is ideal for JEE questions because:**
- âœ… Fast enough for real-time use (6-9s vs 13-15s)
- âœ… Excellent at OCR and math/science content
- âœ… Lower cost (10x cheaper than gpt-4o)
- âœ… Still uses OpenAI's latest training data
- âš ï¸  Slightly less detailed explanations (acceptable trade-off)

#### Implementation:

**File: `backend/src/services/openai.js` (line 77)**

```javascript
// BEFORE:
const response = await openai.chat.completions.create({
  model: "gpt-4o",  // â† Current
  messages: [...]
});

// AFTER:
const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",  // â† Faster, still accurate
  messages: [...]
});
```

**Testing Checklist:**
- [ ] Test 10 simple questions (verify accuracy)
- [ ] Test 10 complex questions (diagrams, multi-part)
- [ ] Compare solution quality vs gpt-4o
- [ ] Measure actual response time (expect 6-9s)

**Rollback Plan:**
If accuracy drops significantly, revert to `gpt-4o`.

**Expected Result:**
- Before: 13,393ms (gpt-4o)
- After: 7,000-9,000ms (gpt-4o-mini)
- **Savings: 4,400-6,400ms (25-38%)**

---

### **Priority 3: Parallel Storage Upload + OpenAI Call** ğŸ”§ ADVANCED
**Expected Savings: 500ms (3%)**
**Effort: 2 hours**
**Risk: Medium**

#### Current Flow:
```javascript
1. Upload to Firebase Storage  â†’ 543ms
2. Call OpenAI API             â†’ 13,393ms
3. Save to Firestore           â†’ 2,358ms
Total: 16,294ms
```

#### Optimized Flow:
```javascript
1. Start: Upload to storage + Call OpenAI (parallel)
   â”œâ”€ Storage upload: 543ms
   â””â”€ OpenAI API: 13,393ms
2. Wait for both to complete: max(543ms, 13,393ms) = 13,393ms
3. Save to Firestore: 2,358ms
Total: 15,751ms
Savings: 543ms
```

#### Implementation:

**File: `backend/src/routes/solve.js`**

```javascript
// Start both operations in parallel
const storageStartTime = Date.now();
const [imageUrl, solutionData] = await Promise.all([
  // Upload to storage
  (async () => {
    const filename = `snaps/${userId}/${uuidv4()}_${req.file.originalname}`;
    const file = storage.bucket().file(filename);
    await file.save(imageBuffer, {
      metadata: {
        contentType: req.file.mimetype,
        metadata: { userId, requestId: req.id }
      }
    });
    return `gs://${storage.bucket().name}/${filename}`;
  })(),

  // Call OpenAI
  Promise.race([
    solveQuestionFromImage(imageBuffer),
    setTimeoutPromise(120000).then(() => {
      throw new ApiError(504, 'Request timeout...');
    })
  ])
]);
perfSteps.parallelOperations = Date.now() - storageStartTime;
```

**Note:** This saves ~500ms but adds complexity. Recommended after Priority 1 & 2.

---

### **Priority 4: Move Render to Singapore** ğŸŒ LONG-TERM
**Expected Savings: 300-600ms (2-4%)**
**Effort: 30 minutes**
**Risk: Low**

#### Current Setup:
- Backend: Render US-East
- Firebase: Mumbai (India)
- Users: India
- Latency: US-East â†” Mumbai = ~250ms per operation

#### After Moving to Singapore:
- Backend: Render Singapore
- Firebase: Mumbai (India)
- Users: India
- Latency: Singapore â†” Mumbai = ~30-50ms per operation

**Firestore Impact:**
- Save snap record: 2,358ms â†’ 1,900ms (save ~450ms)
- Usage check: 1,107ms â†’ 900ms (save ~200ms)
- **Total savings: ~650ms**

**Implementation Steps:**
1. Go to Render dashboard
2. Service Settings â†’ Change region to "Singapore"
3. Redeploy
4. Test performance

---

## ğŸ“ˆ Combined Impact Projection

### Phase 1: Quick Wins (This Week)
**Implement Priority 1 + Priority 2**

```
Before: 17,403ms (17.4 seconds)
â”œâ”€ Firebase Storage: 543ms (3.1%)
â”œâ”€ OpenAI API: 13,393ms (77.0%)
â”œâ”€ Firestore save: 2,358ms (13.5%)
â””â”€ Usage check: 1,107ms (6.4%)

After Phase 1: 10,800ms (10.8 seconds) - 38% FASTER
â”œâ”€ Firebase Storage: 543ms (5.0%)
â”œâ”€ OpenAI API: 7,500ms (69.4%) â† gpt-4o-mini
â”œâ”€ Firestore operations: 2,400ms (22.2%) â† parallel
â””â”€ Overhead: 357ms (3.3%)

Savings: 6,600ms (38% improvement) âœ… MEETS TARGET
```

### Phase 2: Long-Term Optimization (Next Week)
**Add Priority 3 + Priority 4**

```
After Phase 2: 9,600ms (9.6 seconds) - 45% FASTER
â”œâ”€ Firebase Storage: (hidden in parallel)
â”œâ”€ OpenAI API: 7,500ms (78.1%)
â”œâ”€ Firestore operations: 1,750ms (18.2%) â† Singapore region
â””â”€ Overhead: 350ms (3.6%)

Total Savings: 7,800ms (45% improvement) âœ… EXCEEDS TARGET
```

---

## ğŸš€ Implementation Timeline

### Week 1: Quick Wins
**Day 1:**
- âœ… Implement Priority 1 (Parallel Firestore)
- âœ… Deploy and test
- âœ… Measure improvement

**Day 2:**
- âœ… Implement Priority 2 (GPT-4o-mini)
- âœ… Test accuracy on 20 questions
- âœ… Deploy if acceptable

**Day 3:**
- âœ… Monitor production performance
- âœ… Collect user feedback on solution quality

### Week 2: Advanced Optimization
**Day 1:**
- âš ï¸ Implement Priority 3 (Parallel storage + OpenAI)
- âš ï¸ Test thoroughly

**Day 2:**
- ğŸŒ Move Render to Singapore (Priority 4)
- ğŸŒ Verify performance improvement

---

## ğŸ§ª Testing Checklist

### After Each Optimization:
- [ ] Deploy to Render
- [ ] Perform 5 test snaps
- [ ] Check backend logs for new performance breakdown
- [ ] Verify solution quality (especially after model change)
- [ ] Measure actual vs expected improvement

### Success Criteria:
- âœ… Total time: <12 seconds (target: 8-12s)
- âœ… Solution quality: Same or better than current
- âœ… No increase in error rate
- âœ… User satisfaction maintained

---

## ğŸ“Š Risk Assessment

| Optimization | Risk Level | Rollback Difficulty |
|--------------|-----------|-------------------|
| Priority 1 (Parallel Firestore) | Low | Easy (git revert) |
| Priority 2 (GPT-4o-mini) | Medium | Easy (1 line change) |
| Priority 3 (Parallel storage) | Medium | Medium (code restructure) |
| Priority 4 (Singapore region) | Low | Easy (region change) |

---

## ğŸ’¡ Key Insights

1. **OpenAI is still the bottleneck** (77% of time)
   - Switching to gpt-4o-mini will have biggest impact
   - Need to test accuracy carefully

2. **Firestore operations are unexpectedly slow** (20% of time)
   - Due to US-East â†’ Mumbai latency
   - Parallel operations + Singapore region will help significantly

3. **Firebase Storage is not an issue** (3% of time)
   - 543ms is acceptable
   - No optimization needed

4. **Combined optimizations can achieve 45% improvement**
   - Phase 1 alone gets us to target (38% improvement)
   - Phase 2 adds additional polish (45% improvement)

---

## ğŸ¯ Recommendation

**Start with Phase 1 (Priority 1 + Priority 2):**
1. Implement parallel Firestore operations (low risk, quick win)
2. Switch to GPT-4o-mini and test thoroughly
3. Deploy if accuracy is acceptable
4. Monitor for 2-3 days
5. Proceed to Phase 2 if stable

**Expected outcome:** 17.4s â†’ 10.8s (38% faster, meets target)

---

## ğŸ“ Next Steps

1. **Review this plan** - Any questions or concerns?
2. **Approve Priority 1** - Parallel Firestore operations
3. **Approve Priority 2** - GPT-4o-mini testing
4. **Start implementation** - I can implement both changes now
5. **Deploy and measure** - Track actual improvement

Ready to proceed? ğŸš€
